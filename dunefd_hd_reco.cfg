[global]
group      = dune
experiment = dune
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
version = override_me
quals   = override_me
fclfile = override_me
outdir = override_me
tarname = override_me
tardir = override_me

[env_pass]
SAM_EXPERIMENT=%(experiment)s
SAM_GROUP=%(group)s
SAM_STATION=%(experiment)s
IFDH_CP_MAXRETRIES=2
XRD_CONNECTIONRETRY=32
XRD_REQUESTTIMEOUT=14400
XRD_REDIRECTLIMIT=255
XRD_LOADBALANCERTTL=7200
XRD_STREAMTIMEOUT=14400


[submit]
G	= %(group)s
N	= 1
resource-provides = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE
memory		        = 2000MB
cpu               = 1
disk		          = 5GB
expected-lifetime = 12h
append_condor_requirements = \(TARGET.HAS_Singularity==true\&\&TARGET.HAS_CVMFS_dune_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_larsoft_opensciencegrid_org==true\&\&TARGET.CVMFS_dune_opensciencegrid_org_REVISION\>=1105\&\&TARGET.HAS_CVMFS_fifeuser1_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_fifeuser2_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_fifeuser3_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_fifeuser4_opensciencegrid_org==true\)
lines_1    = +SingularityImage=\\\"/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:latest\\\"
lines_2    = +FERMIHTC_AutoRelease=True
lines_3    = +FERMIHTC_GraceMemory=2000
lines_4    = +FERMIHTC_GraceLifetime=21600
tar_file_name = dropbox://%(tardir)s/%(tarname)s
use-cvmfs-dropbox = True

[job_setup]
#debug       = True                                                                                                                                                                                                     
find_setups = True
# For custom code tarball submission one of these, suitably altered
#source_1    = ${INPUT_TAR_DIR_LOCAL}/setupMay2021Tutorial-grid.sh
##source_2    = ./may2021tutorial/localProducts_larsoft_%(version)s_e19_prof/setup-grid

# For non-tarball submission, these two lines instead
source_1 = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh
setup_1 = %(experiment)stpc %(version)s -q %(quals)s

multifile   = False

export_1 = FILETIMESTAMP=$(date -u +%%Y%%m%%dT%%H%%M%%SZ)
export_2 = FHICL_FILE_PATH=.:$FHICL_FILE_PATH
export_3 = FW_SEARCH_PATH=.:$FW_SEARCH_PATH

# could be any valid bash lines/commands, or another script
prescript_1 = cd ${_CONDOR_JOB_IWD} 
prescript_2 = ln -s ${INPUT_TAR_DIR_LOCAL}/PandoraSettings_Write.xml .

# if your fcl file is in a release or other location in FW_SEARCH_PATH you don't need these steps.
# the grid sets up the local tar directory for you via the Rapid Code Distribution Service if using cvmfs dropbox, as in this example
prescript_3 = ln -s ${INPUT_TAR_DIR_LOCAL}/%(fclfile)s .
prescript_4 = ln -s ${INPUT_TAR_DIR_LOCAL}/run_std.fcl .

# if your fcl file is in a release or other location in FW_SEARCH_PATH you don't need this step.
#prescript_5 = ln -s ${INPUT_TAR_DIR_LOCAL}/may2021tutorial/work/%(fclfile)s .
prescript_5 = ifdh cp -D root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/persistent/users/chappell/run_1x2x6.fcl .
prescript_6 = ifdh cp -D root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/persistent/users/chappell/run_std.fcl .

# If you expect to read more than one input file per job you may need this set to True
ifdh_art = False

[sam_consumer]
limit = 1
schema = root
appvers = %(version)s

[executable]
name  = lar
arg_1 = -c
arg_2 = %(fclfile)s
arg_3 = -n
arg_4 = 5
#arg_5 = -T
#arg_6 = tutorial_hist_\${CLUSTER}_\${PROCESS}_\\\\\\\${FILETIMESTAMP}.root

[job_output]
addoutput = nu_dune10kt_*_reco.root
rename      = unique
dest      = %(outdir)s
declare_metadata = False
metadata_extractor = json
add_location = False

[job_output_1]
addoutput = Pandora_*.pndr
rename      = unique
dest      = %(outdir)s
declare_metadata = False
metadata_extractor = json
add_location = False

[job_output_2]
addoutput = Pandora_Geometry.xml
#rename      = unique
dest      = %(outdir)s
declare_metadata = False
metadata_extractor = json
add_location = False
