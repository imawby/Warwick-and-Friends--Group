[global]
group      = dune
experiment = dune
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
version = override_me
quals   = override_me
fclfile = override_me
outdir = override_me
tarname = override_me
tardir = override_me

[env_pass]
SAM_EXPERIMENT=%(experiment)s
SAM_GROUP=%(group)s
SAM_STATION=%(experiment)s
IFDH_CP_MAXRETRIES=2
XRD_CONNECTIONRETRY=32
XRD_REQUESTTIMEOUT=14400
XRD_REDIRECTLIMIT=255
XRD_LOADBALANCERTTL=7200
XRD_STREAMTIMEOUT=14400


[submit]
G	= %(group)s
N	= 1
resource-provides = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE
memory		  = 2000MB
cpu           = 1
disk		  = 2GB
expected-lifetime = 2h
append_condor_requirements = \(TARGET.HAS_Singularity==true\&\&TARGET.HAS_CVMFS_dune_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_larsoft_opensciencegrid_org==true\&\&TARGET.CVMFS_dune_opensciencegrid_org_REVISION\>=1105\&\&TARGET.HAS_CVMFS_fifeuser1_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_fifeuser2_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_fifeuser3_opensciencegrid_org==true\&\&TARGET.HAS_CVMFS_fifeuser4_opensciencegrid_org==true\)
lines_1    = +SingularityImage=\\\"/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:latest\\\"
lines_2    = +FERMIHTC_AutoRelease=True
lines_3    = +FERMIHTC_GraceMemory=2000
lines_4    = +FERMIHTC_GraceLifetime=7200
#tar_file_name = dropbox://%(tardir)s/%(tarname)s
use-cvmfs-dropbox = True

[job_setup]
#debug       = True                                                                                                                                                                                                     
find_setups = True
# For tarball submission one of these
#source_1    = ${INPUT_TAR_DIR_LOCAL}/setupMay2021Tutorial-grid.sh
##source_2    = ./may2021tutorial/localProducts_larsoft_%(version)s_e19_prof/setup-grid

# For non-tarball submission, these two lines instead
source_1 = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh
setup_1 = %(experiment)stpc %(version)s -q %(quals)s

multifile   = False

# could be any valid bash lines/commands, or another script
prescript_1 = cd ${_CONDOR_JOB_IWD} 
prescript_2 = ifdh cp -D root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/persistent/users/chappell/PandoraSettings_Write.xml .
prescript_3 = export FW_SEARCH_PATH=.:$FW_SEARCH_PATH
prescript_4 = export FHICL_FILE_PATH=.:$FHICL_FILE_PATH

# if your fcl file is in a release or other location in FW_SEARCH_PATH you don't need this step.
#prescript_5 = ln -s ${INPUT_TAR_DIR_LOCAL}/may2021tutorial/work/%(fclfile)s .
prescript_5 = ifdh cp -D root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/persistent/users/chappell/run_1x2x6.fcl .
prescript_6 = ifdh cp -D root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/persistent/users/chappell/run_std.fcl .

export_1 = FILETIMESTAMP=$(date -u +%%Y%%m%%dT%%H%%M%%SZ)
# If you expect to read more than one input file per job you may need this set to True
ifdh_art = False

[sam_consumer]
limit = 1
schema = root
appvers = %(version)s

[executable]
name  = lar
arg_1 = -c
arg_2 = %(fclfile)s
arg_3 = -n
arg_4 = 5
#arg_5 = -T
#arg_6 = tutorial_hist_\${CLUSTER}_\${PROCESS}_\\\\\\\${FILETIMESTAMP}.root

[job_output]
addoutput = nu_dune10kt_*_reco.root
rename      = unique
dest      = %(outdir)s
declare_metadata = False
metadata_extractor = json
add_location = False

[job_output_1]
addoutput = Pandora_*.pndr
rename      = unique
dest      = %(outdir)s
declare_metadata = False
metadata_extractor = json
add_location = False

[job_output_2]
addoutput = Pandora_Geometry.xml
#rename      = unique
dest      = %(outdir)s
declare_metadata = False
metadata_extractor = json
add_location = False
